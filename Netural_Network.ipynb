{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neutral Network in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "# Setting random seeds to get reproducible results\n",
    "np.random.seed(0)\n",
    "import tensorflow as tf\n",
    "# tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./helping/Chapter_10_Neural_Networks/one_circle.csv', index_col=0)\n",
    "x = np.array(df[['x_1', 'x_2']])\n",
    "y = np.array(df['y']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.759416</td>\n",
       "      <td>2.753240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.885278</td>\n",
       "      <td>1.629527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.463302</td>\n",
       "      <td>-1.023869</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.986004</td>\n",
       "      <td>-0.898810</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.010834</td>\n",
       "      <td>-2.580117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>-1.376637</td>\n",
       "      <td>2.778703</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-0.703722</td>\n",
       "      <td>0.215382</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.729767</td>\n",
       "      <td>-2.479655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>-1.715920</td>\n",
       "      <td>-0.393404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2.382873</td>\n",
       "      <td>-2.951074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          x_1       x_2  y\n",
       "0   -0.759416  2.753240  0\n",
       "1   -1.885278  1.629527  0\n",
       "2    2.463302 -1.023869  0\n",
       "3   -1.986004 -0.898810  0\n",
       "4    2.010834 -2.580117  0\n",
       "..        ...       ... ..\n",
       "105 -1.376637  2.778703  1\n",
       "106 -0.703722  0.215382  1\n",
       "107  0.729767 -2.479655  1\n",
       "108 -1.715920 -0.393404  1\n",
       "109  2.382873 -2.951074  0\n",
       "\n",
       "[110 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In tensorflow we need to convert our label into binary form.\n",
    "# Categorizing the output\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "categorized_y = np.array(to_categorical(y, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 8,770\n",
      "Trainable params: 8,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "#import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "#from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "#from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Building the model\n",
    "model = Sequential()\n",
    "# First Hidden layer with 128 nodes and with relu activation function.\n",
    "model.add(Dense(128, activation='relu', input_shape=(2,)))\n",
    "model.add(Dropout(.2))\n",
    "# Second hidden layer with 64 hidden nodes and relu activation function.. \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "# Output layer with softmax activation layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compiling the model\n",
    "# Here loss is like a log-loss function. but here is more then one column thats why we used categorial loss\n",
    "# Optimizar is built-in tricks by keras package.\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "110/110 [==============================] - 0s 3ms/sample - loss: 0.5018 - acc: 0.7727\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - 0s 189us/sample - loss: 0.4734 - acc: 0.7636\n",
      "Epoch 3/100\n",
      "110/110 [==============================] - 0s 243us/sample - loss: 0.4535 - acc: 0.7636\n",
      "Epoch 4/100\n",
      "110/110 [==============================] - 0s 194us/sample - loss: 0.4577 - acc: 0.7636\n",
      "Epoch 5/100\n",
      "110/110 [==============================] - 0s 407us/sample - loss: 0.4353 - acc: 0.7636\n",
      "Epoch 6/100\n",
      "110/110 [==============================] - 0s 499us/sample - loss: 0.4408 - acc: 0.7636\n",
      "Epoch 7/100\n",
      "110/110 [==============================] - 0s 190us/sample - loss: 0.4316 - acc: 0.7636\n",
      "Epoch 8/100\n",
      "110/110 [==============================] - 0s 288us/sample - loss: 0.4436 - acc: 0.7636\n",
      "Epoch 9/100\n",
      "110/110 [==============================] - 0s 149us/sample - loss: 0.4194 - acc: 0.7636\n",
      "Epoch 10/100\n",
      "110/110 [==============================] - 0s 157us/sample - loss: 0.4219 - acc: 0.7636\n",
      "Epoch 11/100\n",
      "110/110 [==============================] - 0s 231us/sample - loss: 0.4231 - acc: 0.7636\n",
      "Epoch 12/100\n",
      "110/110 [==============================] - 0s 234us/sample - loss: 0.4037 - acc: 0.7636\n",
      "Epoch 13/100\n",
      "110/110 [==============================] - 0s 181us/sample - loss: 0.4057 - acc: 0.7636\n",
      "Epoch 14/100\n",
      "110/110 [==============================] - 0s 175us/sample - loss: 0.3991 - acc: 0.7727\n",
      "Epoch 15/100\n",
      "110/110 [==============================] - 0s 172us/sample - loss: 0.3887 - acc: 0.7818\n",
      "Epoch 16/100\n",
      "110/110 [==============================] - 0s 187us/sample - loss: 0.3868 - acc: 0.7818\n",
      "Epoch 17/100\n",
      "110/110 [==============================] - 0s 378us/sample - loss: 0.3665 - acc: 0.7818\n",
      "Epoch 18/100\n",
      "110/110 [==============================] - 0s 300us/sample - loss: 0.3778 - acc: 0.7818\n",
      "Epoch 19/100\n",
      "110/110 [==============================] - 0s 170us/sample - loss: 0.3754 - acc: 0.8000\n",
      "Epoch 20/100\n",
      "110/110 [==============================] - 0s 172us/sample - loss: 0.3711 - acc: 0.7909\n",
      "Epoch 21/100\n",
      "110/110 [==============================] - 0s 171us/sample - loss: 0.3955 - acc: 0.8091\n",
      "Epoch 22/100\n",
      "110/110 [==============================] - 0s 171us/sample - loss: 0.3948 - acc: 0.8091\n",
      "Epoch 23/100\n",
      "110/110 [==============================] - 0s 154us/sample - loss: 0.3602 - acc: 0.8091\n",
      "Epoch 24/100\n",
      "110/110 [==============================] - 0s 159us/sample - loss: 0.3753 - acc: 0.8091\n",
      "Epoch 25/100\n",
      "110/110 [==============================] - 0s 186us/sample - loss: 0.3580 - acc: 0.8273\n",
      "Epoch 26/100\n",
      "110/110 [==============================] - 0s 138us/sample - loss: 0.3595 - acc: 0.8455\n",
      "Epoch 27/100\n",
      "110/110 [==============================] - 0s 178us/sample - loss: 0.3429 - acc: 0.8364\n",
      "Epoch 28/100\n",
      "110/110 [==============================] - 0s 124us/sample - loss: 0.3477 - acc: 0.8455\n",
      "Epoch 29/100\n",
      "110/110 [==============================] - 0s 179us/sample - loss: 0.3305 - acc: 0.8273\n",
      "Epoch 30/100\n",
      "110/110 [==============================] - 0s 182us/sample - loss: 0.3367 - acc: 0.8545\n",
      "Epoch 31/100\n",
      "110/110 [==============================] - 0s 162us/sample - loss: 0.3363 - acc: 0.8818\n",
      "Epoch 32/100\n",
      "110/110 [==============================] - 0s 156us/sample - loss: 0.3434 - acc: 0.8636\n",
      "Epoch 33/100\n",
      "110/110 [==============================] - 0s 161us/sample - loss: 0.3536 - acc: 0.8636\n",
      "Epoch 34/100\n",
      "110/110 [==============================] - 0s 232us/sample - loss: 0.3136 - acc: 0.9000\n",
      "Epoch 35/100\n",
      "110/110 [==============================] - 0s 179us/sample - loss: 0.3204 - acc: 0.8818\n",
      "Epoch 36/100\n",
      "110/110 [==============================] - 0s 206us/sample - loss: 0.3190 - acc: 0.9000\n",
      "Epoch 37/100\n",
      "110/110 [==============================] - 0s 199us/sample - loss: 0.2936 - acc: 0.8909\n",
      "Epoch 38/100\n",
      "110/110 [==============================] - 0s 189us/sample - loss: 0.3043 - acc: 0.8818\n",
      "Epoch 39/100\n",
      "110/110 [==============================] - 0s 206us/sample - loss: 0.3240 - acc: 0.8818\n",
      "Epoch 40/100\n",
      "110/110 [==============================] - 0s 227us/sample - loss: 0.3132 - acc: 0.8818\n",
      "Epoch 41/100\n",
      "110/110 [==============================] - 0s 197us/sample - loss: 0.3142 - acc: 0.8909\n",
      "Epoch 42/100\n",
      "110/110 [==============================] - 0s 196us/sample - loss: 0.2974 - acc: 0.9091\n",
      "Epoch 43/100\n",
      "110/110 [==============================] - 0s 198us/sample - loss: 0.2958 - acc: 0.8636\n",
      "Epoch 44/100\n",
      "110/110 [==============================] - 0s 173us/sample - loss: 0.2985 - acc: 0.9000\n",
      "Epoch 45/100\n",
      "110/110 [==============================] - 0s 226us/sample - loss: 0.2814 - acc: 0.8909\n",
      "Epoch 46/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4321 - acc: 0.900 - 0s 189us/sample - loss: 0.3327 - acc: 0.8818\n",
      "Epoch 47/100\n",
      "110/110 [==============================] - 0s 224us/sample - loss: 0.2747 - acc: 0.9000\n",
      "Epoch 48/100\n",
      "110/110 [==============================] - 0s 279us/sample - loss: 0.2789 - acc: 0.9091\n",
      "Epoch 49/100\n",
      "110/110 [==============================] - 0s 248us/sample - loss: 0.2972 - acc: 0.9000\n",
      "Epoch 50/100\n",
      "110/110 [==============================] - 0s 317us/sample - loss: 0.3139 - acc: 0.9182\n",
      "Epoch 51/100\n",
      "110/110 [==============================] - 0s 128us/sample - loss: 0.2829 - acc: 0.8909\n",
      "Epoch 52/100\n",
      "110/110 [==============================] - 0s 381us/sample - loss: 0.2658 - acc: 0.9182\n",
      "Epoch 53/100\n",
      "110/110 [==============================] - 0s 278us/sample - loss: 0.2754 - acc: 0.9182\n",
      "Epoch 54/100\n",
      "110/110 [==============================] - 0s 232us/sample - loss: 0.2625 - acc: 0.9000\n",
      "Epoch 55/100\n",
      "110/110 [==============================] - 0s 251us/sample - loss: 0.2629 - acc: 0.8818\n",
      "Epoch 56/100\n",
      "110/110 [==============================] - 0s 199us/sample - loss: 0.2831 - acc: 0.8818\n",
      "Epoch 57/100\n",
      "110/110 [==============================] - 0s 359us/sample - loss: 0.2701 - acc: 0.9273\n",
      "Epoch 58/100\n",
      "110/110 [==============================] - 0s 232us/sample - loss: 0.2428 - acc: 0.9091\n",
      "Epoch 59/100\n",
      "110/110 [==============================] - 0s 225us/sample - loss: 0.2782 - acc: 0.8909\n",
      "Epoch 60/100\n",
      "110/110 [==============================] - 0s 373us/sample - loss: 0.3111 - acc: 0.9000\n",
      "Epoch 61/100\n",
      "110/110 [==============================] - 0s 248us/sample - loss: 0.2895 - acc: 0.8818\n",
      "Epoch 62/100\n",
      "110/110 [==============================] - 0s 185us/sample - loss: 0.2689 - acc: 0.9000\n",
      "Epoch 63/100\n",
      "110/110 [==============================] - 0s 200us/sample - loss: 0.2719 - acc: 0.9364\n",
      "Epoch 64/100\n",
      "110/110 [==============================] - 0s 176us/sample - loss: 0.2727 - acc: 0.8909\n",
      "Epoch 65/100\n",
      "110/110 [==============================] - 0s 224us/sample - loss: 0.2486 - acc: 0.9000\n",
      "Epoch 66/100\n",
      "110/110 [==============================] - 0s 320us/sample - loss: 0.2561 - acc: 0.8909\n",
      "Epoch 67/100\n",
      "110/110 [==============================] - 0s 197us/sample - loss: 0.2559 - acc: 0.9182\n",
      "Epoch 68/100\n",
      "110/110 [==============================] - 0s 176us/sample - loss: 0.2889 - acc: 0.9091\n",
      "Epoch 69/100\n",
      "110/110 [==============================] - 0s 187us/sample - loss: 0.2524 - acc: 0.8909\n",
      "Epoch 70/100\n",
      "110/110 [==============================] - 0s 189us/sample - loss: 0.2590 - acc: 0.8909\n",
      "Epoch 71/100\n",
      "110/110 [==============================] - 0s 195us/sample - loss: 0.2420 - acc: 0.9091\n",
      "Epoch 72/100\n",
      "110/110 [==============================] - 0s 173us/sample - loss: 0.2685 - acc: 0.9273\n",
      "Epoch 73/100\n",
      "110/110 [==============================] - 0s 221us/sample - loss: 0.2381 - acc: 0.9091\n",
      "Epoch 74/100\n",
      "110/110 [==============================] - 0s 176us/sample - loss: 0.2069 - acc: 0.9273\n",
      "Epoch 75/100\n",
      "110/110 [==============================] - 0s 168us/sample - loss: 0.2553 - acc: 0.9182\n",
      "Epoch 76/100\n",
      "110/110 [==============================] - 0s 193us/sample - loss: 0.2622 - acc: 0.9000\n",
      "Epoch 77/100\n",
      "110/110 [==============================] - 0s 218us/sample - loss: 0.2289 - acc: 0.9091\n",
      "Epoch 78/100\n",
      "110/110 [==============================] - 0s 169us/sample - loss: 0.2680 - acc: 0.8818\n",
      "Epoch 79/100\n",
      "110/110 [==============================] - 0s 200us/sample - loss: 0.2646 - acc: 0.8818\n",
      "Epoch 80/100\n",
      "110/110 [==============================] - 0s 178us/sample - loss: 0.2521 - acc: 0.9000\n",
      "Epoch 81/100\n",
      "110/110 [==============================] - 0s 221us/sample - loss: 0.2608 - acc: 0.9000\n",
      "Epoch 82/100\n",
      "110/110 [==============================] - 0s 187us/sample - loss: 0.2677 - acc: 0.9000\n",
      "Epoch 83/100\n",
      "110/110 [==============================] - 0s 183us/sample - loss: 0.2444 - acc: 0.8818\n",
      "Epoch 84/100\n",
      "110/110 [==============================] - 0s 234us/sample - loss: 0.2588 - acc: 0.8909\n",
      "Epoch 85/100\n",
      "110/110 [==============================] - 0s 149us/sample - loss: 0.2357 - acc: 0.8818\n",
      "Epoch 86/100\n",
      "110/110 [==============================] - 0s 166us/sample - loss: 0.2211 - acc: 0.9182\n",
      "Epoch 87/100\n",
      "110/110 [==============================] - 0s 204us/sample - loss: 0.2309 - acc: 0.9273\n",
      "Epoch 88/100\n",
      "110/110 [==============================] - 0s 198us/sample - loss: 0.2576 - acc: 0.9182\n",
      "Epoch 89/100\n",
      "110/110 [==============================] - 0s 174us/sample - loss: 0.2390 - acc: 0.9364\n",
      "Epoch 90/100\n",
      "110/110 [==============================] - 0s 195us/sample - loss: 0.2573 - acc: 0.9000\n",
      "Epoch 91/100\n",
      "110/110 [==============================] - 0s 205us/sample - loss: 0.2378 - acc: 0.8818\n",
      "Epoch 92/100\n",
      "110/110 [==============================] - 0s 371us/sample - loss: 0.2773 - acc: 0.9000\n",
      "Epoch 93/100\n",
      "110/110 [==============================] - 0s 206us/sample - loss: 0.2141 - acc: 0.9273\n",
      "Epoch 94/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0958 - acc: 1.000 - 0s 165us/sample - loss: 0.2326 - acc: 0.9000\n",
      "Epoch 95/100\n",
      "110/110 [==============================] - 0s 385us/sample - loss: 0.2567 - acc: 0.8818\n",
      "Epoch 96/100\n",
      "110/110 [==============================] - 0s 168us/sample - loss: 0.2004 - acc: 0.9182\n",
      "Epoch 97/100\n",
      "110/110 [==============================] - 0s 359us/sample - loss: 0.2070 - acc: 0.9455\n",
      "Epoch 98/100\n",
      "110/110 [==============================] - 0s 174us/sample - loss: 0.2466 - acc: 0.8818\n",
      "Epoch 99/100\n",
      "110/110 [==============================] - 0s 200us/sample - loss: 0.2629 - acc: 0.8818\n",
      "Epoch 100/100\n",
      "110/110 [==============================] - 0s 171us/sample - loss: 0.2370 - acc: 0.8727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d5e7992f98>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(x, categorized_y, epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.759416 ,  2.7532401])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 8,770\n",
      "Trainable params: 8,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "model = keras.models.Sequential()\n",
    "model.add(Dense(128,activation='tanh',input_shape=(2,)))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "110/110 [==============================] - 0s 2ms/sample - loss: 0.5797 - acc: 0.7091\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - 0s 149us/sample - loss: 0.5204 - acc: 0.7364\n",
      "Epoch 3/100\n",
      "110/110 [==============================] - 0s 148us/sample - loss: 0.4523 - acc: 0.7636\n",
      "Epoch 4/100\n",
      "110/110 [==============================] - 0s 69us/sample - loss: 0.4471 - acc: 0.7636\n",
      "Epoch 5/100\n",
      "110/110 [==============================] - 0s 104us/sample - loss: 0.4621 - acc: 0.7636\n",
      "Epoch 6/100\n",
      "110/110 [==============================] - 0s 98us/sample - loss: 0.4485 - acc: 0.7636\n",
      "Epoch 7/100\n",
      "110/110 [==============================] - 0s 72us/sample - loss: 0.4694 - acc: 0.7636\n",
      "Epoch 8/100\n",
      "110/110 [==============================] - 0s 119us/sample - loss: 0.4511 - acc: 0.7636\n",
      "Epoch 9/100\n",
      "110/110 [==============================] - 0s 52us/sample - loss: 0.4233 - acc: 0.7636\n",
      "Epoch 10/100\n",
      "110/110 [==============================] - 0s 119us/sample - loss: 0.4556 - acc: 0.7636\n",
      "Epoch 11/100\n",
      "110/110 [==============================] - 0s 110us/sample - loss: 0.4501 - acc: 0.7727\n",
      "Epoch 12/100\n",
      "110/110 [==============================] - 0s 73us/sample - loss: 0.4380 - acc: 0.7636\n",
      "Epoch 13/100\n",
      "110/110 [==============================] - 0s 58us/sample - loss: 0.4379 - acc: 0.7636\n",
      "Epoch 14/100\n",
      "110/110 [==============================] - 0s 116us/sample - loss: 0.4398 - acc: 0.7636\n",
      "Epoch 15/100\n",
      "110/110 [==============================] - 0s 106us/sample - loss: 0.4291 - acc: 0.7636\n",
      "Epoch 16/100\n",
      "110/110 [==============================] - 0s 130us/sample - loss: 0.4568 - acc: 0.7636\n",
      "Epoch 17/100\n",
      "110/110 [==============================] - 0s 104us/sample - loss: 0.4459 - acc: 0.7636\n",
      "Epoch 18/100\n",
      "110/110 [==============================] - 0s 94us/sample - loss: 0.4337 - acc: 0.7636\n",
      "Epoch 19/100\n",
      "110/110 [==============================] - 0s 80us/sample - loss: 0.4408 - acc: 0.7636\n",
      "Epoch 20/100\n",
      "110/110 [==============================] - 0s 106us/sample - loss: 0.4401 - acc: 0.7727\n",
      "Epoch 21/100\n",
      "110/110 [==============================] - 0s 108us/sample - loss: 0.4369 - acc: 0.7636\n",
      "Epoch 22/100\n",
      "110/110 [==============================] - 0s 135us/sample - loss: 0.4046 - acc: 0.7636\n",
      "Epoch 23/100\n",
      "110/110 [==============================] - 0s 145us/sample - loss: 0.4237 - acc: 0.7727\n",
      "Epoch 24/100\n",
      "110/110 [==============================] - 0s 538us/sample - loss: 0.4233 - acc: 0.7727\n",
      "Epoch 25/100\n",
      "110/110 [==============================] - 0s 73us/sample - loss: 0.4219 - acc: 0.7727\n",
      "Epoch 26/100\n",
      "110/110 [==============================] - 0s 167us/sample - loss: 0.4016 - acc: 0.8000\n",
      "Epoch 27/100\n",
      "110/110 [==============================] - 0s 73us/sample - loss: 0.4078 - acc: 0.7818\n",
      "Epoch 28/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5987 - acc: 0.656 - 0s 73us/sample - loss: 0.4219 - acc: 0.7818\n",
      "Epoch 29/100\n",
      "110/110 [==============================] - 0s 79us/sample - loss: 0.4056 - acc: 0.7818\n",
      "Epoch 30/100\n",
      "110/110 [==============================] - 0s 82us/sample - loss: 0.4080 - acc: 0.8000\n",
      "Epoch 31/100\n",
      "110/110 [==============================] - 0s 177us/sample - loss: 0.3971 - acc: 0.7909\n",
      "Epoch 32/100\n",
      "110/110 [==============================] - 0s 166us/sample - loss: 0.4009 - acc: 0.7727\n",
      "Epoch 33/100\n",
      "110/110 [==============================] - 0s 127us/sample - loss: 0.3843 - acc: 0.8091\n",
      "Epoch 34/100\n",
      "110/110 [==============================] - 0s 98us/sample - loss: 0.4008 - acc: 0.8091\n",
      "Epoch 35/100\n",
      "110/110 [==============================] - 0s 147us/sample - loss: 0.3965 - acc: 0.7818\n",
      "Epoch 36/100\n",
      "110/110 [==============================] - 0s 117us/sample - loss: 0.3971 - acc: 0.8000\n",
      "Epoch 37/100\n",
      "110/110 [==============================] - 0s 72us/sample - loss: 0.3792 - acc: 0.8091\n",
      "Epoch 38/100\n",
      "110/110 [==============================] - 0s 149us/sample - loss: 0.4009 - acc: 0.8000\n",
      "Epoch 39/100\n",
      "110/110 [==============================] - 0s 105us/sample - loss: 0.3621 - acc: 0.8182\n",
      "Epoch 40/100\n",
      "110/110 [==============================] - 0s 131us/sample - loss: 0.3688 - acc: 0.8364\n",
      "Epoch 41/100\n",
      "110/110 [==============================] - 0s 81us/sample - loss: 0.3895 - acc: 0.8000\n",
      "Epoch 42/100\n",
      "110/110 [==============================] - 0s 83us/sample - loss: 0.3468 - acc: 0.8273\n",
      "Epoch 43/100\n",
      "110/110 [==============================] - 0s 122us/sample - loss: 0.3642 - acc: 0.8182\n",
      "Epoch 44/100\n",
      "110/110 [==============================] - 0s 86us/sample - loss: 0.3730 - acc: 0.8273\n",
      "Epoch 45/100\n",
      "110/110 [==============================] - 0s 100us/sample - loss: 0.3720 - acc: 0.8273\n",
      "Epoch 46/100\n",
      "110/110 [==============================] - 0s 94us/sample - loss: 0.3652 - acc: 0.8182\n",
      "Epoch 47/100\n",
      "110/110 [==============================] - 0s 119us/sample - loss: 0.3396 - acc: 0.8636\n",
      "Epoch 48/100\n",
      "110/110 [==============================] - 0s 85us/sample - loss: 0.3331 - acc: 0.8636\n",
      "Epoch 49/100\n",
      "110/110 [==============================] - 0s 86us/sample - loss: 0.3683 - acc: 0.8091\n",
      "Epoch 50/100\n",
      "110/110 [==============================] - 0s 104us/sample - loss: 0.3505 - acc: 0.8545\n",
      "Epoch 51/100\n",
      "110/110 [==============================] - 0s 53us/sample - loss: 0.3438 - acc: 0.8545\n",
      "Epoch 52/100\n",
      "110/110 [==============================] - 0s 107us/sample - loss: 0.3430 - acc: 0.8364\n",
      "Epoch 53/100\n",
      "110/110 [==============================] - 0s 119us/sample - loss: 0.3320 - acc: 0.8727\n",
      "Epoch 54/100\n",
      "110/110 [==============================] - 0s 75us/sample - loss: 0.3440 - acc: 0.8818\n",
      "Epoch 55/100\n",
      "110/110 [==============================] - 0s 83us/sample - loss: 0.3667 - acc: 0.8545\n",
      "Epoch 56/100\n",
      "110/110 [==============================] - 0s 70us/sample - loss: 0.3363 - acc: 0.8727\n",
      "Epoch 57/100\n",
      "110/110 [==============================] - 0s 124us/sample - loss: 0.3472 - acc: 0.8545\n",
      "Epoch 58/100\n",
      "110/110 [==============================] - 0s 129us/sample - loss: 0.3207 - acc: 0.9182\n",
      "Epoch 59/100\n",
      "110/110 [==============================] - 0s 86us/sample - loss: 0.3272 - acc: 0.8455\n",
      "Epoch 60/100\n",
      "110/110 [==============================] - 0s 102us/sample - loss: 0.3423 - acc: 0.8545\n",
      "Epoch 61/100\n",
      "110/110 [==============================] - 0s 114us/sample - loss: 0.3215 - acc: 0.8727\n",
      "Epoch 62/100\n",
      "110/110 [==============================] - 0s 110us/sample - loss: 0.3653 - acc: 0.8545\n",
      "Epoch 63/100\n",
      "110/110 [==============================] - 0s 80us/sample - loss: 0.3333 - acc: 0.8727\n",
      "Epoch 64/100\n",
      "110/110 [==============================] - 0s 84us/sample - loss: 0.3567 - acc: 0.8636\n",
      "Epoch 65/100\n",
      "110/110 [==============================] - 0s 75us/sample - loss: 0.3162 - acc: 0.8818\n",
      "Epoch 66/100\n",
      "110/110 [==============================] - 0s 81us/sample - loss: 0.3312 - acc: 0.8727\n",
      "Epoch 67/100\n",
      "110/110 [==============================] - 0s 126us/sample - loss: 0.3050 - acc: 0.8909\n",
      "Epoch 68/100\n",
      "110/110 [==============================] - 0s 83us/sample - loss: 0.2817 - acc: 0.8545\n",
      "Epoch 69/100\n",
      "110/110 [==============================] - 0s 72us/sample - loss: 0.3223 - acc: 0.8727\n",
      "Epoch 70/100\n",
      "110/110 [==============================] - 0s 80us/sample - loss: 0.3006 - acc: 0.8818\n",
      "Epoch 71/100\n",
      "110/110 [==============================] - 0s 87us/sample - loss: 0.3147 - acc: 0.8818\n",
      "Epoch 72/100\n",
      "110/110 [==============================] - 0s 112us/sample - loss: 0.3089 - acc: 0.8727\n",
      "Epoch 73/100\n",
      "110/110 [==============================] - 0s 75us/sample - loss: 0.3378 - acc: 0.9000\n",
      "Epoch 74/100\n",
      "110/110 [==============================] - 0s 100us/sample - loss: 0.3005 - acc: 0.8818\n",
      "Epoch 75/100\n",
      "110/110 [==============================] - 0s 132us/sample - loss: 0.2990 - acc: 0.8818\n",
      "Epoch 76/100\n",
      "110/110 [==============================] - 0s 67us/sample - loss: 0.2891 - acc: 0.9182\n",
      "Epoch 77/100\n",
      "110/110 [==============================] - 0s 101us/sample - loss: 0.3152 - acc: 0.8636\n",
      "Epoch 78/100\n",
      "110/110 [==============================] - 0s 107us/sample - loss: 0.3131 - acc: 0.8727\n",
      "Epoch 79/100\n",
      "110/110 [==============================] - 0s 84us/sample - loss: 0.3287 - acc: 0.9000\n",
      "Epoch 80/100\n",
      "110/110 [==============================] - 0s 77us/sample - loss: 0.3006 - acc: 0.8909\n",
      "Epoch 81/100\n",
      "110/110 [==============================] - 0s 82us/sample - loss: 0.3222 - acc: 0.8455\n",
      "Epoch 82/100\n",
      "110/110 [==============================] - 0s 88us/sample - loss: 0.2947 - acc: 0.8909\n",
      "Epoch 83/100\n",
      "110/110 [==============================] - 0s 87us/sample - loss: 0.2982 - acc: 0.8909\n",
      "Epoch 84/100\n",
      "110/110 [==============================] - 0s 97us/sample - loss: 0.3267 - acc: 0.8818\n",
      "Epoch 85/100\n",
      "110/110 [==============================] - 0s 86us/sample - loss: 0.2995 - acc: 0.8909\n",
      "Epoch 86/100\n",
      "110/110 [==============================] - 0s 415us/sample - loss: 0.2972 - acc: 0.9091\n",
      "Epoch 87/100\n",
      "110/110 [==============================] - 0s 108us/sample - loss: 0.2980 - acc: 0.9000\n",
      "Epoch 88/100\n",
      "110/110 [==============================] - 0s 177us/sample - loss: 0.3154 - acc: 0.9091\n",
      "Epoch 89/100\n",
      "110/110 [==============================] - 0s 72us/sample - loss: 0.2935 - acc: 0.8818\n",
      "Epoch 90/100\n",
      "110/110 [==============================] - 0s 382us/sample - loss: 0.2873 - acc: 0.9091\n",
      "Epoch 91/100\n",
      "110/110 [==============================] - 0s 105us/sample - loss: 0.3186 - acc: 0.8636\n",
      "Epoch 92/100\n",
      "110/110 [==============================] - 0s 93us/sample - loss: 0.2741 - acc: 0.8909\n",
      "Epoch 93/100\n",
      "110/110 [==============================] - 0s 93us/sample - loss: 0.3095 - acc: 0.8909\n",
      "Epoch 94/100\n",
      "110/110 [==============================] - 0s 284us/sample - loss: 0.3204 - acc: 0.9091\n",
      "Epoch 95/100\n",
      "110/110 [==============================] - 0s 135us/sample - loss: 0.2676 - acc: 0.9091\n",
      "Epoch 96/100\n",
      "110/110 [==============================] - 0s 65us/sample - loss: 0.2870 - acc: 0.9000\n",
      "Epoch 97/100\n",
      "110/110 [==============================] - 0s 69us/sample - loss: 0.2636 - acc: 0.9091\n",
      "Epoch 98/100\n",
      "110/110 [==============================] - 0s 75us/sample - loss: 0.3166 - acc: 0.8545\n",
      "Epoch 99/100\n",
      "110/110 [==============================] - 0s 105us/sample - loss: 0.3310 - acc: 0.9091\n",
      "Epoch 100/100\n",
      "110/110 [==============================] - 0s 125us/sample - loss: 0.2855 - acc: 0.9091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d5e9cfc438>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x,categorized_y,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label is 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[1000], cmap='Greys')\n",
    "print(\"The label is\", y_train[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x1440 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "for i in range(5):\n",
    "    ax = fig.add_subplot(1, 5, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(x_train[i], cmap='Greys')\n",
    "    ax.set_title('Label:' + str(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the features.\n",
    "# In the reshape function we use the -1 as a placeholder for the size of the dataset.\n",
    "\n",
    "x_train_reshaped = x_train.reshape(-1, 28*28)\n",
    "x_test_reshaped = x_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_cat = to_categorical(y_train, 10)\n",
    "y_test_cat = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "# create Model ...\n",
    "model = keras.models.Sequential()\n",
    "model.add(Dense(128,activation='relu',input_shape=(28*28,)))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 10s 166us/sample - loss: 2.1241 - acc: 0.4736\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 10s 162us/sample - loss: 0.8625 - acc: 0.7397\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 176us/sample - loss: 0.6512 - acc: 0.8171 - loss: 0\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 10s 161us/sample - loss: 0.5447 - acc: 0.8486\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 10s 159us/sample - loss: 0.4825 - acc: 0.8742\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 10s 160us/sample - loss: 0.4381 - acc: 0.8918\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 10s 161us/sample - loss: 0.3935 - acc: 0.9029\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 10s 165us/sample - loss: 0.3892 - acc: 0.9061\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 10s 165us/sample - loss: 0.3712 - acc: 0.9084\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 10s 172us/sample - loss: 0.3613 - acc: 0.9126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18197317438>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_reshaped,y_train_cat,epochs=10,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_vector = model.predict(x_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [np.argmax(pred) for pred in predictions_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32), 7)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_vector[0],predictions[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is correct 9501 times out of 10000\n",
      "The accuracy is 0.9501\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == y_test[i]:\n",
    "        num_correct += 1\n",
    "\n",
    "print(\"The model is correct\", num_correct, \"times out of\", len(y_test))\n",
    "print(\"The accuracy is\", num_correct/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
